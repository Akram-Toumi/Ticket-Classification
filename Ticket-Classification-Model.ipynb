{"cells":[{"source":"![servicedesk](servicedesk.png)\n\nCleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can automatically categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as mortgage, credit card, money transfers, debt collection, etc.","metadata":{"executionCancelledAt":null,"executionTime":165,"lastExecutedAt":1707667023665,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"CleverSupport is a company at the forefront of AI innovation, specializing in the development of AI-driven solutions to enhance customer support services. Their latest endeavor is to engineer a text classification system that can autonomously categorize customer complaints. \n\nYour role as a data scientist involves the creation of a sophisticated machine learning model that can accurately assign complaints to specific categories, such as technical issues, billing inquiries, cancellation requests, refunds, and product information requests."},"id":"e5870ae0-6165-459e-9c40-0f282883be7b","cell_type":"markdown"},{"source":"!pip install torchmetrics","metadata":{"executionCancelledAt":null,"executionTime":3042,"lastExecutedAt":1734018666200,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7"},"id":"0dd4beb4-2329-4b0d-8a34-2354ee9c7fb4","cell_type":"code","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchmetrics in /home/repl/.local/lib/python3.8/site-packages (1.5.2)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /home/repl/.local/lib/python3.8/site-packages (from torchmetrics) (0.11.9)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"}]},{"source":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall\nimport torch.optim as optim","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1734018666254,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from collections import Counter\nimport nltk, json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall\nimport torch.optim as optim","lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7"},"id":"2fa90b61-0244-4236-aa93-e33a7a088eec","cell_type":"code","execution_count":11,"outputs":[]},{"source":"nltk.download('punkt')","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1734018666307,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"nltk.download('punkt')","outputsMetadata":{"0":{"height":80,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false},"lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7"},"id":"37a51a81-1301-4a80-b8c6-716faaff4c5c","cell_type":"code","execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to /home/repl/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":12}]},{"source":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')","metadata":{"executionCancelledAt":null,"executionTime":106,"lastExecutedAt":1734018666413,"lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import data and labels\nwith open(\"words.json\", 'r') as f1:\n    words = json.load(f1)\nwith open(\"text.json\", 'r') as f2:\n    text = json.load(f2)\nlabels = np.load('labels.npy')"},"id":"e1b12eaf-e55c-422c-94a2-b0197c465a1b","cell_type":"code","execution_count":13,"outputs":[]},{"source":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)","metadata":{"executionCancelledAt":null,"executionTime":263,"lastExecutedAt":1734018666677,"lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}\n\n# Looking up the mapping dictionary and assigning the index to the respective words\nfor i, sentence in enumerate(text):\n    text[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n    \n# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\ntext = pad_input(text, 50)"},"id":"d630badb-23dd-4368-9a96-e2b478ad5cff","cell_type":"code","execution_count":14,"outputs":[]},{"source":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1734018666726,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Splitting dataset\ntrain_text, test_text, train_label, test_label = train_test_split(text, labels, test_size=0.2, random_state=42)\n\ntrain_data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_label).long())\ntest_data = TensorDataset(torch.from_numpy(test_text), torch.from_numpy(test_label).long())","lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7"},"id":"f2654836-631f-415e-9922-5ab3bafaaafa","cell_type":"code","execution_count":15,"outputs":[]},{"source":"# Start coding here","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1734018666774,"lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here"},"id":"d2b3c50c-66b1-40ed-a17c-038e7addc7ec","cell_type":"code","execution_count":16,"outputs":[]},{"source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import Accuracy, Precision, Recall\nimport numpy as np\n\n# DataLoader parameters\nbatch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n\n# Define the TicketClassifier class\nclass TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)  # Adjust dimensions for Conv1d\n        conved = F.relu(self.conv(embedded))\n        pooled = conved.mean(dim=2)  # Global average pooling\n        return self.fc(pooled)\n\n# Model parameters\nvocab_size = len(word2idx) + 1\nembedding_dim = 64\ntarget_size = len(np.unique(labels))\n\n# Initialize model, loss, and optimizer\nmodel = TicketClassifier(vocab_size, embedding_dim, target_size)\nlr = 0.05\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# Training loop\nepochs = 3\nmodel.train()\nprint(\"Starting Training...\")\nfor epoch in range(epochs):\n    running_loss, num_processed = 0.0, 0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)  # Accumulate total loss\n        num_processed += inputs.size(0)\n\n    epoch_loss = running_loss / num_processed\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n\n# Evaluation metrics\naccuracy_metric = Accuracy(task='multiclass', num_classes=target_size)\nprecision_metric = Precision(task='multiclass', num_classes=target_size, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=target_size, average=None)\n\n# Evaluation loop\nmodel.eval()\npredicted = []\nprint(\"Starting Evaluation...\")\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        output = model(inputs)\n        predictions = torch.argmax(output, dim=1)\n        predicted.extend(predictions.tolist())\n\n        # Update metrics\n        accuracy_metric.update(predictions, labels)\n        precision_metric.update(predictions, labels)\n        recall_metric.update(predictions, labels)\n\n# Compute and display metrics\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"Precision (per class):\", precision)\nprint(\"Recall (per class):\", recall)\n","metadata":{"executionCancelledAt":null,"executionTime":1948,"lastExecutedAt":1734018668722,"lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchmetrics.classification import Accuracy, Precision, Recall\nimport numpy as np\n\n# DataLoader parameters\nbatch_size = 400\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n\n# Define the TicketClassifier class\nclass TicketClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, target_size):\n        super(TicketClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n        self.fc = nn.Linear(embed_dim, target_size)\n\n    def forward(self, text):\n        embedded = self.embedding(text).permute(0, 2, 1)  # Adjust dimensions for Conv1d\n        conved = F.relu(self.conv(embedded))\n        pooled = conved.mean(dim=2)  # Global average pooling\n        return self.fc(pooled)\n\n# Model parameters\nvocab_size = len(word2idx) + 1\nembedding_dim = 64\ntarget_size = len(np.unique(labels))\n\n# Initialize model, loss, and optimizer\nmodel = TicketClassifier(vocab_size, embedding_dim, target_size)\nlr = 0.05\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n# Training loop\nepochs = 3\nmodel.train()\nprint(\"Starting Training...\")\nfor epoch in range(epochs):\n    running_loss, num_processed = 0.0, 0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)  # Accumulate total loss\n        num_processed += inputs.size(0)\n\n    epoch_loss = running_loss / num_processed\n    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n\n# Evaluation metrics\naccuracy_metric = Accuracy(task='multiclass', num_classes=target_size)\nprecision_metric = Precision(task='multiclass', num_classes=target_size, average=None)\nrecall_metric = Recall(task='multiclass', num_classes=target_size, average=None)\n\n# Evaluation loop\nmodel.eval()\npredicted = []\nprint(\"Starting Evaluation...\")\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        output = model(inputs)\n        predictions = torch.argmax(output, dim=1)\n        predicted.extend(predictions.tolist())\n\n        # Update metrics\n        accuracy_metric.update(predictions, labels)\n        precision_metric.update(predictions, labels)\n        recall_metric.update(predictions, labels)\n\n# Compute and display metrics\naccuracy = accuracy_metric.compute().item()\nprecision = precision_metric.compute().tolist()\nrecall = recall_metric.compute().tolist()\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"Precision (per class):\", precision)\nprint(\"Recall (per class):\", recall)\n","outputsMetadata":{"0":{"height":185,"type":"stream"}}},"cell_type":"code","id":"f410856a-9634-4796-8abe-21fc6a702bd5","outputs":[{"output_type":"stream","name":"stdout","text":"Starting Training...\nEpoch [1/3], Loss: 1.5720\nEpoch [2/3], Loss: 0.6701\nEpoch [3/3], Loss: 0.3083\nStarting Evaluation...\nAccuracy: 0.7860\nPrecision (per class): [0.699999988079071, 0.7413793206214905, 0.8421052694320679, 0.7871286869049072, 0.8399999737739563]\nRecall (per class): [0.6927083134651184, 0.678947389125824, 0.8148148059844971, 0.828125, 0.8999999761581421]\n"}],"execution_count":17},{"source":"print(\"Accuracy:\",accuracy)\nprint(\"Precision:\",precision)\nprint(\"Recall:\",recall)","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1734018837966,"lastExecutedByKernel":"3fd008d3-3741-4156-aac2-05e9cb167cb7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(\"Accuracy:\",accuracy)\nprint(\"Precision:\",precision)\nprint(\"Recall:\",recall)","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"fba0a462-67e3-4155-ab1d-8a6e8021eadd","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.7860000133514404\nPrecision: [0.699999988079071, 0.7413793206214905, 0.8421052694320679, 0.7871286869049072, 0.8399999737739563]\nRecall: [0.6927083134651184, 0.678947389125824, 0.8148148059844971, 0.828125, 0.8999999761581421]\n"}],"execution_count":24}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}